<!DOCTYPE html>
<html lang="pt-BR">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>üå∏ Flor que segue a palma</title>

  <!-- A-Frame + AR.js -->
  <script src="https://aframe.io/releases/1.3.0/aframe.min.js"></script>
  <script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar.js"></script>

  <!-- MediaPipe Hands -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

  <style>
    html, body { margin: 0; overflow: hidden; }
    a-scene {
      position: fixed;
      top: 0; left: 0;
      width: 100vw; height: 100vh;
    }
    #arjs-video {
      filter: brightness(1.15) contrast(1.1) saturate(1.2);
      object-fit: cover;
      transform: scaleX(-1);
    }
  </style>
</head>

<body>
  <a-scene embedded arjs="sourceType: webcam; videoTexture: true; debugUIEnabled: false;">
    <a-entity id="model"
      gltf-model="url(assets/florzinha.glb)"
      visible="false"
      position="0 0 -2"
      scale="0.01 0.01 0.01">
    </a-entity>
    <a-entity camera></a-entity>
  </a-scene>

  <script>
    const hands = new Hands({ locateFile: f => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${f}` });
    hands.setOptions({
      maxNumHands: 1,
      modelComplexity: 1,
      minDetectionConfidence: 0.7,
      minTrackingConfidence: 0.6
    });

    const model = document.getElementById('model');
    const video = document.querySelector('video');

    video.addEventListener('loadeddata', () => {
      console.log('‚úÖ C√¢mera AR.js pronta');
      processFrames();
    });

    async function processFrames() {
      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');

      async function loop() {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        await hands.send({ image: canvas });
        requestAnimationFrame(loop);
      }

      function getOpenness(hand) {
        const base = hand[0];
        const tip = hand[12];
        const dist = Math.sqrt(
          Math.pow(tip.x - base.x, 2) +
          Math.pow(tip.y - base.y, 2) +
          Math.pow(tip.z - base.z, 2)
        );
        return dist;
      }

      let currentScale = 0.01;

      hands.onResults(results => {
        if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
          const hand = results.multiHandLandmarks[0];
          const openness = getOpenness(hand);

          // faixa de abertura da m√£o
          const min = 0.05;
          const max = 0.25;
          let t = (openness - min) / (max - min);
          t = Math.max(0, Math.min(1, t));

          // calcula escala
          const targetScale = 0.05 + t * 2.0;
          currentScale += (targetScale - currentScale) * 0.15;

          // pega o ponto central da palma
          const palm = hand[9];

          // converte posi√ß√£o da palma para coordenadas da cena (ajuste de eixo)
          const x = (palm.x - 0.5) * -4;  // horizontal invertido
          const y = (0.5 - palm.y) * 3;   // vertical
          const z = -2 + palm.z * 5;      // profundidade

          // atualiza posi√ß√£o e tamanho
          model.setAttribute('position', `${x} ${y} ${z}`);
          model.setAttribute('scale', `${currentScale} ${currentScale} ${currentScale}`);

          // se m√£o fechada, esconde
          if (t < 0.1) {
            model.setAttribute('visible', false);
          } else {
            model.setAttribute('visible', true);
          }

        } else {
          model.setAttribute('visible', false);
        }
      });

      loop();
    }
  </script>
</body>
</html>
